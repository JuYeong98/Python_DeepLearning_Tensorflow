{"metadata":{"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 디렉토리와 jpeg 파일명을 읽어서 파일의 절대경로 위치와 학습/테스트 데이터 셋 여부, 해당 이미지의 label 값을 설정 ","metadata":{}},{"cell_type":"code","source":"paths = []\ndataset_gubuns = []\nlabel_gubuns = []\n# os.walk()를 이용하여 특정 디렉토리 밑에 있는 모든 하위 디렉토리를 모두 조사. \n# cat-and-dog 하위 디렉토리 밑에 jpg 확장자를 가진 파일이 모두 이미지 파일임\n# cat-and-dog 밑으로 /train/, /test/ 하위 디렉토리 존재(학습, 테스트 용 이미지 파일들을 가짐)\n\nfor dirname, _, filenames in os.walk('/kaggle/input/cat-and-dog'):\n    for filename in filenames:\n        # 이미지 파일이 아닌 파일도 해당 디렉토리에 있음.\n        if '.jpg' in filename:\n            # 파일의 절대 경로를 file_path 변수에 할당. \n            file_path = dirname+'/'+ filename\n            paths.append(file_path)\n            # 파일의 절대 경로에 training_set, test_set가 포함되어 있으면 데이터 세트 구분을 'train'과 'test'로 분류. \n            if '/training_set/' in file_path:\n                dataset_gubuns.append('train')  \n            elif '/test_set/' in file_path:\n                dataset_gubuns.append('test')\n            else: dataset_gubuns.append('N/A')\n            \n            # 파일의 절대 경로에 dogs가 있을 경우 해당 파일은 dog 이미지 파일이고, cats일 경우는 cat 이미지 파일임. \n            if 'dogs' in file_path:\n                label_gubuns.append('DOG')\n            elif 'cats' in file_path:\n                label_gubuns.append('CAT')\n            else: label_gubuns.append('N/A')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paths[:10] , dataset_gubuns[:10], label_gubuns[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 위에서 생성된 이미지 파일의 절대 경로, 데이터 세트 구분, Lable값을 DataFrame으로 생성. ","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', 200)\n\ndata_df = pd.DataFrame({'path':paths, 'dataset':dataset_gubuns, 'label':label_gubuns})\nprint('data_df shape:', data_df.shape)\ndata_df.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 데이터 세트값 분포 및 Label 값 분포를 확인한다. \nprint(data_df['dataset'].value_counts())\nprint(data_df['label'].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DOG와 CAT의 이미지 파일 절대 경로를 cv2.imread()로 읽어서 image array로 로드하고 이미지 시각화\n# 이미지별로 서로 다른 이미지 사이즈를 가지고 있음. \nimport matplotlib.pyplot as plt\nimport cv2\n%matplotlib inline \n\ndef show_grid_images(image_path_list, ncols=8, augmentor=None, title=None):\n    figure, axs = plt.subplots(figsize=(22, 6), nrows=1, ncols=ncols)\n    for i in range(ncols):\n        image = cv2.cvtColor(cv2.imread(image_path_list[i]), cv2.COLOR_BGR2RGB)\n        axs[i].imshow(image)\n        axs[i].set_title(title)  \n        \ndog_image_list = data_df[data_df['label']=='DOG']['path'].iloc[:6].tolist()\nshow_grid_images(dog_image_list, ncols=6, title='DOG')\n\ncat_image_list = data_df[data_df['label']=='CAT']['path'].iloc[:6].tolist()\nshow_grid_images(cat_image_list, ncols=6, title='CAT')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for image_path in dog_image_list:\n    image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n    print('image shape:', image.shape)\n    \nprint('image height shape:', image[:, 0, 0].shape)\nprint('image width shape:', image[0, :, 0].shape)\nprint('image size:', image[:, :, 0].shape)\n\nprint('### image array:\\n', image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocessing 과 Data Loading 메커니즘\n* (초기) Keras는 Preprocessing과 Data Loading을 ImageDataGenerator 객체와 model의 fit_generator()가 상호 연결되어 수행. \n* model의 fit_generator()가 인자로 ImageDataGenerator가 생성한 generator 객체를 입력 받아서 image 파일부터 Model에 array/Tensor값 입력까지 Pipeline Stream으로 이어지게 구성. \n* generator 를 통해서 model에 Tensor로 공급하는 로직은 아래와 같음. \n* image 파일을 image array로 로드-> preprocessing 적용(augmentation -> array 값 scale 조정(0~1사이 값) -> array 크기 resize-> Normalization 적용)\n* Label값 인코딩(문자열을 단일 숫자값/원-핫 인코딩) 또한 generator기반으로 손쉽게 변경해줌.\n* 실제 Preprocessing과 Data Loading은 Model에서 fit_generator()를 호출하기 전까지는 수행되지 않음. \n\n\n### Preprocessing\n* image file을 image array로 변환 시 0 ~ 1사이의 float형(float32)으로 변경. \n* image array size를 고정 크기로 재 조정.(예를 들어 224 x 224 )\n* augmentation 적용.\n* Normalization(평균과 표준 편차 재 조정)\n* Label값은 binary classification 이냐 multiple classification에 따라 숫자형 값 Encoding. \n* multiple classification일 경우 Label Encoding을 One-hot encoding 할 지 결정. \n\n### Data Loading\n* os에 있는 image 파일을 메모리로 array 형태로 로딩. \n* 이때 대량의 image 파일을 메모리로 loading 할 경우 메모리가 감당할 수 없으므로 일정 크기 단위로 array 변환. 즉 BATCH 크기 단위로 array 로딩. ","metadata":{}},{"cell_type":"markdown","source":"### ImageDataGenerator 객체 생성\n* ImageDataGenerator 객체 생성시 인자로 augmentation과 rescale에 대한 인자값을 입력하여 Preprocessing 환경 설정. \n\n### ImageDataGenerator 객체의 flow_from_directory(), flow_from_dataframe() 호출\n* 생성된 ImageDataGenerator 객체의 flow_from_directory() 메소드를 호출하여 Data Loading 환경 설정. \n* flow_from_directory()는 인자로 입력된 directory에 해당하는 디렉토리에 위치한 서브 디렉토리등에서 모든 image 파일을 검색. \n* 이때 바로 아래 서브 디렉토리의 이름을 Label로 인식하고, 해당 서브 디렉토리에 있는 이미지들을 해당 Label에 속하는 이미지 파일로 간주\n* class_mode로 Label Encoding을 자동으로 수행. \n    * class_mode='categorical' 일 경우 문자열 Label을 원-핫 인코딩\n    * class_mode='sparse' 일 경우 문자열 Label을 숫자값으로 인코딩\n    * class_mode='binary' 일 경우 문자열 Label을 0/1 숫자값으로 인코딩\n* batch_size로 한번에 읽어올 이미지 파일 갯수 설정(즉 Batch 크기 설정).\n    * 모든 이미지 파일을 한번에 Numpy Array로 변환할 경우 메모리가 부족하게 되므로 수십개 단위로 이미지 파일을 한번에 읽어옴.\n* target_size로 Model에 입력하기 위해서 원본 이미지의 크기를 변경할 resize 설정. \n* shuffle은 읽어온 데이터를 원래 순서대로가 아닌 섞어서 전달할지 여부\n    ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n### Preprocessing과 Data Loading을 동시에 적용. 단 실제 Preprocessing과 Data Loading을 Model에서 fit_generator()를 호출하기 전까지는 동작하지 않음. \n# augmentation은 horizontal_flip(좌우 반전)만 적용하고 0 ~ 255의 pixel값을 0 ~ 1 로 scale만 적용. \ntrain_gen = ImageDataGenerator(horizontal_flip=True, rescale=1/255.)\n\n# ImageDataGenerator 객체의 flow_from_directory() 메소드를 호출. \n# class_mode='categorical' 로 Label 데이터를 원-핫 인코딩, 이미지 array는 224 x 224 로 변경. Batch 크기는 64로 설정. \ntrain_flow_gen = train_gen.flow_from_directory(directory='/kaggle/input/cat-and-dog/training_set/training_set' # image file이 있는 디렉토리 \n                                                         ,target_size=(224, 224) # 원본 이미지를 최종 resize할 image size\n                                                         ,class_mode='categorical' # 문자열 label을 자동으로 one-hot encoding 시켜줌. \n                                                         ,batch_size=64\n                                                         )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### next()를 이용하여 flow_from_directory()로 반환된 Iterator 객체를 강제로 호출하여 어떤 값이 만들어 지는지 확인.  ","metadata":{}},{"cell_type":"code","source":"# next()를 flow_from_directory()로 반환된 Iterator 객체를 강제로 호출하여 어떤 값이 만들어 지는지 확인.  \n# 튜플형태로 반환되고 튜플의 첫번째값은 image array, 두번째 값은 label array 임\n# image array는 0~1 사이 값인 float32로 변환됨. label array는 2개 column으로 원-핫 인코딩 됨. \n# next(iter(train_flow_gen))\nimages_array, labels_array = next(train_flow_gen)\nprint(images_array.shape, labels_array.shape)\nprint(images_array[:1], labels_array[:1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image array와 label array의 shape 확인. \n# batch_size만큼 한꺼번에 추출되므로 image array는 4차원, label array는 batch_size와 원-핫 인코딩을 감안하여 2차원 데이터\nimages_array = next(train_flow_gen)[0]\nlabels_array = next(train_flow_gen)[1]\nprint('##### image array shape:', images_array.shape)\nprint('#### label array shape:', labels_array.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# label array가 원-핫 인코딩 되어 있는지 확인.\nlabels_array","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model을 생성\n* Input의 shape는 VGG, ResNet, Xception 모델이 원래 학습 이미지 사이즈로 가졌던 224 x 224 로 설정.   \n* 이를 위해 flow_from_directory()로 원본 이미지를 224 x 224 로 변환함.  ","metadata":{}},{"cell_type":"code","source":"IMAGE_SIZE = 224\nBATCH_SIZE = 64","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Dense , Conv2D , Dropout , Flatten , Activation, MaxPooling2D , GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam , RMSprop \nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications import ResNet50V2\nfrom tensorflow.keras.applications import Xception\n\ndef create_model(model_name='vgg16', verbose=False):\n    \n    input_tensor = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n    if model_name == 'vgg16':\n        base_model = VGG16(input_tensor=input_tensor, include_top=False, weights='imagenet')\n    elif model_name == 'resnet50':\n        base_model = ResNet50V2(input_tensor=input_tensor, include_top=False, weights='imagenet')\n    elif model_name == 'xception':\n        base_model = Xception(input_tensor=input_tensor, include_top=False, weights='imagenet')\n    \n    bm_output = base_model.output\n\n    x = GlobalAveragePooling2D()(bm_output)\n    if model_name != 'vgg16':\n        x = Dropout(rate=0.5)(x)\n    x = Dense(50, activation='relu', name='fc1')(x)\n    # 개와 고양이 2 종류이므로 Dense의 units는 2\n    output = Dense(2, activation='softmax', name='output')(x)\n\n    model = Model(inputs=input_tensor, outputs=output)\n    \n    if verbose:\n        model.summary()\n        \n    return model\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### optimizer, loss, metric 설정\n* 위에서 flow_from_directory(class_mode='categorical') 로 설정했기에 label은 원핫 인코딩 되어 모델로 입력됨\n* 따라서 loss는 categorical_crossentropy 를 적용해야 함. ","metadata":{}},{"cell_type":"code","source":"# 위에서 flow_from_directory(class_mode='categorical') 로 설정했기에 label은 원핫 인코딩 되어 모델로 입력됨\n# loss 는 원핫 인코딩을 기반으로 계산이 될 수 있는 loss 함수 필요. 여기서는 categorical_crossentropy를 써야함\n# 만약 위에서 class_mode='sparse' 라면 loss는 sparse_categorical_crossentropy 여야 함.\nmodel = create_model(model_name='xception', verbose=True)\nmodel.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 모델의 fit_generator()를 호출하여 학습 수행. \n* fit_generator()는 앞에서 생성한 iterator 객체를 인자로 받아서 image array, label array를 배치 크기 만큼 순차적으로 데이터로 가져와 학습 수행.   \n* 입력 인자로 generator는 앞에서 생성한 image_data_generator를 입력\n* epochs는 전체 학습 데이터를 가져와서 학습하는 횟수 의미\n* steps_per_epoch는 전체 학습 데이터를 몇번 배치 작업으로 수행하는가를 의미. generator는 배치크기를 알수 있지만 전체 데이터에서 이를 batch 만큼 데이터를 몇번 가져와야 전체 데이터 학습을 수행할 지 알 수 없으므로 이를 설정해 줌. 전체 데이터 건수/배치 크기이며, 소수점 이하는 무조건 +1 로 절삭. 즉 전체 데이터 1001개 이고, 배치 크기가 10이면 steps_per_epochs는 101 임. ","metadata":{}},{"cell_type":"code","source":"# train 데이터 건수는 generator의 samples 속성을 이용하여 가져올 수 있음. \ntrain_image_cnt = train_flow_gen.samples \nprint(train_image_cnt)\n# batch size는 64, 학습 데이터 image 개수는 8005 \nmodel.fit_generator(train_flow_gen, epochs=15, \n                    steps_per_epoch=int(np.ceil(train_image_cnt/BATCH_SIZE))\n                   )\n                    \n# 앞으론 fit()을 사용해야함. (fit_generator()와 같이 동작함)\n# model.fit(train_flow_gen, epochs=15, steps_per_epoch=int(np.ceil(train_image_cnt/BATCH_SIZE)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test 데이터로 성능 검증\n* Test를 위한 데이터 Generator 생성. ","metadata":{}},{"cell_type":"code","source":"# test data는 augmentation을 적용할 필요 없음. \ntest_gen = ImageDataGenerator(rescale=1/255.)\ntest_flow_gen = test_gen.flow_from_directory(directory='/kaggle/input/cat-and-dog/test_set/test_set' # image file이 있는 디렉토리 \n                                                         ,target_size=(IMAGE_SIZE, IMAGE_SIZE) # 원본 이미지를 최종 resize할 image size\n                                                         ,class_mode='categorical' # 문자열 label을 자동으로 one-hot encoding 시켜줌. \n                                                         ,batch_size=BATCH_SIZE, shuffle=False\n                                                         )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 모델의 evaluate_generator()를 호출하여 Test 데이터의 성능 측정.(evaluate()도 무방)\nmodel.evaluate_generator(test_flow_gen)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ImageDataGenerator 객체와 flow_from_dataframe()\n* flow_from_dataframe()도 flow_from_directory()와 유사하게 메타 데이터 소스로 부터 이미지 파일과 label을 numpy array로 로드\n* flow_from_dataframe()은 DataFrame 메타 데이터로 부터 로드함. \n* 일반적으로 flow_from_dataframe()이 데이터 조작등이 더 편하기 때문에 flow_from_directory() 보다 더 애용됨. ","metadata":{}},{"cell_type":"markdown","source":"### 이미지 파일의 절대경로 위치와 데이터세트 유형, label값을 가지는 메타 테이블용 DataFrame을 생성. ","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', 200)\n\ndata_df = pd.DataFrame({'path':paths, 'dataset':dataset_gubuns, 'label':label_gubuns})\nprint(data_df['dataset'].value_counts())\ndata_df.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 학습용/검증용/테스트용 DataFrame 을 생성","metadata":{}},{"cell_type":"code","source":"data_df['dataset']=='train'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = data_df[data_df['dataset']=='train']\ntest_df = data_df[data_df['dataset']=='test']\nprint('train_df shape:', train_df.shape, 'test_df shape:', test_df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# scikit learn의 train_test_split()을 이용하여 train용, validation용 DataFrame을 생성\n# stratify를 이용하여 label 값을 균등하게 분할 \ntr_df, val_df = train_test_split(train_df, test_size=0.15, stratify=train_df['label'], random_state=2021)\nprint('tr_df shape:', tr_df.shape, 'val_df shape:', val_df.shape)\nprint('tr_df label distribution:\\n', tr_df['label'].value_counts())\nprint('val_df label distributuion:\\n', val_df['label'].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 학습과 검증용 ImageDataGenerator 생성.\n* flow_from_dataframe()은 인자로 dataframe = 메타 데이터용 DataFrame 입력 받음. \n* x_col은 메타 데이터용 DataFrame에서 image 파일의 절대경로 위치를 나타내는 column명을 의미\n* y_col은 메타 데이터용 DataFrame에서 label값 column명을 의미. 이 때 y_col로 지정된 컬럼은 반드시 문자열(object type)이 되어야 함. \n* y_col의 경우 미리 encoding(label 또는 원핫)을 해서 숫자값으로 절대 변경하면 안됨.  \n*  softmax가 아닌 sigmoid로 최종 출력하므로 class_mode='binary' 로 Label 데이터 변환, 이미지 array는 224 x 224 로 변경. Batch 크기는 64로 설정. ","metadata":{}},{"cell_type":"code","source":"# IMAGE 크기와 BATCH 크기를 위한 환경 변수 설정. \nIMAGE_SIZE = 224\nBATCH_SIZE = 64","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 학습용 ImageDataGenerator 생성과 Data generator 처리","metadata":{}},{"cell_type":"code","source":"\n\n# 학습용과 검증용 ImageDataGenerator 생성. 학습용은 augmentation + rescale적용. 검증용은 rescale만 적용 \n# augmentation은 horizontal_flip(좌우 반전)만 적용하고 0 ~ 255의 pixel값을 0 ~ 1 로 scale만 적용. \ntr_generator = ImageDataGenerator(horizontal_flip=True, rescale=1/255.)\n\n# ImageDataGenerator 객체의 flow_from_dataframe()) 메소드를 호출. \n# flow_from_dataframe()은 인자로 dataframe = 메타 데이터용 DataFrame 입력 받음. \n# x_col은 메타 데이터용 DataFrame에서 image 파일의 절대경로 위치를 나타내는 column명을 의미\n# y_col은 메타 데이터용 DataFrame에서 label값 column명을 의미. 이 때 y_col로 지정된 컬럼은 반드시 문자열(object type)이 되어야 함. \n# y_col의 경우 미리 encoding(label 또는 원핫)을 해서 숫자값으로 절대 변경하면 안됨.  \n# softmax가 아닌 sigmoid로 최종 출력하므로 class_mode='binary' 로 Label 데이터 변환, 이미지 array는 224 x 224 로 변경. Batch 크기는 64로 설정. \ntr_flow_gen = tr_generator.flow_from_dataframe(dataframe=tr_df # image file이 있는 디렉토리\n                                      ,x_col='path'\n                                      ,y_col='label'\n                                      ,target_size=(IMAGE_SIZE, IMAGE_SIZE) # 원본 이미지를 최종 resize할 image size\n                                      ,class_mode='binary' # 문자열 label을 자동 Encoding. \n                                      ,batch_size=BATCH_SIZE\n                                      ,shuffle=True\n                                      )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_array = next(tr_flow_gen)[0]\nlabels_array = next(tr_flow_gen)[1]\nprint('##### image array shape:', images_array.shape)\nprint('#### label array shape:', labels_array.shape)\nprint(images_array[0])\nprint(labels_array[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 검증용 ImageDataGenerator 생성과 Data generator 처리","metadata":{}},{"cell_type":"code","source":"# 검증용 ImageDataGenerator는 rescale만 적용. \nval_generator = ImageDataGenerator(rescale=1/255.)\nval_flow_gen = val_generator.flow_from_dataframe(dataframe=val_df # image file이 있는 디렉토리\n                                      ,x_col='path'\n                                      ,y_col='label'\n                                      ,target_size=(IMAGE_SIZE, IMAGE_SIZE) # 원본 이미지를 최종 resize할 image size\n                                      ,class_mode='binary' # 문자열 label을 자동으로 one-hot encoding 시켜줌. \n                                      ,batch_size=BATCH_SIZE\n                                      ,shuffle=False\n                                      )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 모델 생성","metadata":{}},{"cell_type":"code","source":"def create_model(model_name='vgg16', verbose=False):\n    \n    input_tensor = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n    if model_name == 'vgg16':\n        base_model = VGG16(input_tensor=input_tensor, include_top=False, weights='imagenet')\n    elif model_name == 'resnet50':\n        base_model = ResNet50V2(input_tensor=input_tensor, include_top=False, weights='imagenet')\n    elif model_name == 'xception':\n        base_model = Xception(input_tensor=input_tensor, include_top=False, weights='imagenet')\n    \n    bm_output = base_model.output\n\n    x = GlobalAveragePooling2D()(bm_output)\n    if model_name != 'vgg16':\n        x = Dropout(rate=0.5)(x)\n    x = Dense(50, activation='relu', name='fc1')(x)\n    # 최종 output 출력을 softmax에서 sigmoid로 변환. \n    output = Dense(1, activation='sigmoid', name='output')(x)\n\n    model = Model(inputs=input_tensor, outputs=output)\n    \n    if verbose:\n        model.summary()\n        \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = create_model(model_name='xception')\n# 최종 output 출력을 softmax에서 sigmoid로 변환되었으므로 binary_crossentropy로 변환 \nmodel.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])\n\n# 3번 iteration내에 validation loss가 향상되지 않으면 learning rate을 기존 learning rate * 0.2로 줄임.  \nrlr_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, mode='min', verbose=1)\n# 5번 iteration내에 validation loss가 향상되지 않으면 더 이상 학습하지 않고 종료\nely_cb = EarlyStopping(monitor='val_loss', patience=5, mode='min', verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### fit() 또는 fit_generator() 호출하여 학습 수행.\n* validation_data에는 앞에서 검증용으로 생성한 val_generator를 입력\n* validation_steps는 검증용 데이터에 대한 steps_per_epoch임. ","metadata":{}},{"cell_type":"code","source":"# 학습과 검증용 steps_per_epoch 계산 \nN_EPOCHS = 15\n\nmodel.fit(tr_flow_gen, epochs=N_EPOCHS,\n          steps_per_epoch=int(np.ceil(tr_df.shape[0]/BATCH_SIZE)),\n          validation_data=val_flow_gen, \n          validation_steps=int(np.ceil(val_df.shape[0]/BATCH_SIZE)), \n          callbacks=[rlr_cb, ely_cb])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 테스트용 ImageDataGenerator 생성과 Data generator 처리 후 Evaluation 수행","metadata":{}},{"cell_type":"code","source":"# test data는 augmentation을 적용할 필요 없음. \ntest_generator = ImageDataGenerator(rescale=1/255.)\ntest_flow_gen = test_generator.flow_from_dataframe(dataframe=test_df # image file이 있는 디렉토리\n                                      ,x_col='path'\n                                      ,y_col='label'\n                                      ,target_size=(IMAGE_SIZE, IMAGE_SIZE) # 원본 이미지를 최종 resize할 image size\n                                      ,class_mode='binary' # 문자열 label을 자동으로 encoding. \n                                      ,batch_size=BATCH_SIZE\n                                      ,shuffle=False\n                                      )\n# evaluation으로 성능 검증\nmodel.evaluate(test_flow_gen)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### image data의 scaling preprocessing 유형\n* 0 ~ 1 사이, -1 ~ 1 사이, z-score 변환 중에 선택. \n* pretrained 모델은 주로 tf와 torch 프레임웍으로 생성 \n* tf는 -1 ~ 1 택. torch는 전통적으로 z-score 변환(0 ~ 1로 scaling 후 ImageNet 데이터 세트의 이미지 채널별 평균값, 표준편차값을 이용하여 z score 변환 적용)","metadata":{}},{"cell_type":"code","source":"import cv2\n\n# 임의의 이미지 한개 선택. \nimage = cv2.cvtColor(cv2.imread(data_df['path'].iloc[0]), cv2.COLOR_BGR2RGB)\nplt.imshow(image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### tf 또는 torch 유형으로 변경하는 함수 생성. ","metadata":{}},{"cell_type":"code","source":"def preprocessing_scaling(x, mode='tf'):\n    if mode == 'tf':\n        x = x/127.5\n        x -= 1.\n    \n    elif mode == 'torch':\n        x = x/255.\n        mean = [0.485, 0.456, 0.406]\n        std = [0.229, 0.224, 0.225]\n        \n        x[:, :, 0] = (x[:, :, 0] - mean[0])/std[0]\n        x[:, :, 1] = (x[:, :, 1] - mean[1])/std[1]\n        x[:, :, 2] = (x[:, :, 2] - mean[2])/std[2]\n        \n    return x\n\nscaled_image_tf = preprocessing_scaling(image, mode='tf')\nscaled_image_torch = preprocessing_scaling(image, mode='torch')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### scale 된 이미지 pixel값의 histogram 시각화 ","metadata":{}},{"cell_type":"code","source":"def show_pixel_histogram(image):\n    fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(16, 6))\n\n    for i in range(3):\n        # i값 0 일때 Red 채널, i값 1일때 Green Channel, i값 2일때 Blue Channel Histogram 표현 \n        axs[i].hist(image[:, :, i].flatten(), bins=100, alpha=0.5)\n        axs[i].legend(loc='upper right')\n        if i==0:\n            title_str = 'Red'\n        elif i==1:\n            title_str = 'Green'\n        else: \n            title_str = 'Blue'\n        axs[i].set(title=title_str)\n        \nshow_pixel_histogram(scaled_image_tf)\nshow_pixel_histogram(scaled_image_torch)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_pixel_histogram(image/255.0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 개별 pretrained 모델들은 고유한 scaling 방식을 적용","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications.xception import preprocess_input\n\n# Xception의 scaling 방식은 tf\nscaled_image_xception = preprocess_input(image)\nshow_pixel_histogram(scaled_image_xception)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications.densenet import preprocess_input\n\n# DenseNet의 scaling 방식은 torch\nscaled_image_densenet = preprocess_input(image)\nshow_pixel_histogram(scaled_image_densenet)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ImageDataGenerator에 preprocess_input()을 적용시 preprocessing_function 파라미터를 사용 ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications.xception import preprocess_input\n\n#tr_generator = ImageDataGenerator(horizontal_flip=True, rescale=1/255.)\ntr_generator = ImageDataGenerator(horizontal_flip=True, preprocessing_function=preprocess_input)\ntr_flow_gen = tr_generator.flow_from_dataframe(dataframe=tr_df # image file이 있는 디렉토리\n                                      ,x_col='path'\n                                      ,y_col='label'\n                                      ,target_size=(IMAGE_SIZE, IMAGE_SIZE) # 원본 이미지를 최종 resize할 image size\n                                      ,class_mode='binary' # 문자열 label을 자동 Encoding. \n                                      ,batch_size=BATCH_SIZE\n                                      ,shuffle=True\n                                      )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_array = next(tr_flow_gen)[0]\nlabels_array = next(tr_flow_gen)[1]\n\nshow_pixel_histogram(images_array[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}