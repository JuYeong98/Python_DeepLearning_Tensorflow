{"metadata":{"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CIFAR10 Dataset 생성 \n* tf.keras.datasets의 cifar10.load_data()는 웹에서 Local computer로 Download후 train과 test용 image와 label array로 로딩. ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.datasets import cifar10\n\n# 전체 6만개 데이터 중, 5만개는 학습 데이터용, 1만개는 테스트 데이터용으로 분리\n(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\nprint(\"train dataset shape:\", train_images.shape, train_labels.shape)\nprint(\"test dataset shape:\", test_images.shape, test_labels.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images[0, :, :, :], train_labels[0, :]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NAMES = np.array(['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])\nprint(train_labels[:10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CIFAR10 데이터 시각화\n* 이미지 크기는 32x32이며 RGB채널. \n* 전반적으로 Label에 해당하는 대상이 이미지의 중앙에 있고, Label 대상 오브젝트 위주로 이미지가 구성. ","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport cv2\n%matplotlib inline \n\ndef show_images(images, labels, ncols=8):\n    figure, axs = plt.subplots(figsize=(22, 6), nrows=1, ncols=ncols)\n    for i in range(ncols):\n        axs[i].imshow(images[i])\n        label = labels[i].squeeze()\n        axs[i].set_title(NAMES[int(label)])\n        \nshow_images(train_images[:8], train_labels[:8], ncols=8)\nshow_images(train_images[8:16], train_labels[8:16], ncols=8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data preprocessing\n* image array의 0 ~ 255 사이의 값으로 되어 있는 pixel intensity 값을 0 ~ 1 사이 값으로 변환. 정수값 pixel 값을 255.0 으로 나눔. \n* label array는 숫자형 값으로 바꾸되, 원-핫 인코딩을 적용할지 선택. 일반적으로 원-핫 인코딩을 적용하는게 Keras Framework활용이 용이\n* image array, label array 모두 float32 형으로 변환. numpy 의 float32는 tensor 변환시 tf.float32 로 변환되며 기본적으로 Tensorflow backend Keras는 tf.float32를 기반으로 함. \n","metadata":{}},{"cell_type":"code","source":"(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n\n# label은 원-핫 인코딩이 Keras에서는 활용이 용이하나, 여기서는 sparse categorical crossentropy 테스트를 위해 적용하지 않음. \ndef get_preprocessed_data(images, labels):\n    \n    # 학습과 테스트 이미지 array를 0~1 사이값으로 scale 및 float32 형 변형. \n    images = np.array(images/255.0, dtype=np.float32)\n    labels = np.array(labels, dtype=np.float32)\n    \n    return images, labels\n\ntrain_images, train_labels = get_preprocessed_data(train_images, train_labels)\ntest_images, test_labels = get_preprocessed_data(test_images, test_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images[0, :, :, :]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Keras는 CNN(정확히는 CNN 2D) 모델에 학습 데이터를 입력할 시 반드시 Image array는 4차원 배열이 되어야 함. \n# RGB 채널 이미지 array는 기본적으로 3차원임. 여기에 이미지의 갯수를 포함하므로 4차원이 됨.  \n# 만일 Grayscale인 2차원 이미지 array라도 의도적으로 채널을 명시해서 3차원으로 만들어 주고, 여기에 이미지 개수를 포함해서 4차원이 됨. \n\nprint(train_images.shape, train_labels.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# label 데이터가 2차원임. 이를 Keras 모델에 입력해도 별 문제없이 동작하지만, label의 경우는 OHE적용이 안되었는지를 알 수 있게 명확하게 1차원으로 표현해 주는것이 좋음. \n# 2차원인 labels 데이터를 1차원으로 변경. \ntrain_labels = train_labels.squeeze()\ntest_labels = test_labels.squeeze()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Custom Model 생성\n* CNN Model의 맨처음 Layer는 Input layer. Input layer의 shape를 이미지 사이즈와 RGB 3채널에 맞게 (32, 32, 3) 으로 설정.\n* Conv 연산을 연달아 적용하고 MaxPooling을 적용하는 루틴으로 모델 생성. MaxPooling을 적용 후에는 필터 갯수를 더욱 증가 시킴. \n* MaxPooling 적용 후에 출력 피처맵의 사이즈는 작아지되, 채널(깊이)는 늘어나는 형태로 모델 생성. \n* CIFAR10의 Label수가 10개이므로 Classification을 위한 맨 마지막 Dense layer의 units 갯수는 10개임\n* label값이 원-핫 인코딩 되지 않았기 때문에 model.compile()에서 loss는 반드시 sparse_categorical_crossentropy여야함. \n* 만일 label값이 원-핫 인코딩 되었다면 loss는 categorical_crossentropy 임. ","metadata":{}},{"cell_type":"code","source":"IMAGE_SIZE = 32","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Dense , Conv2D , Dropout , Flatten , Activation, MaxPooling2D , GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam , RMSprop \nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\n\ninput_tensor = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n\n#x = Conv2D(filters=32, kernel_size=(5, 5), padding='valid', activation='relu')(input_tensor)\nx = Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu')(input_tensor)\nx = Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu')(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\nx = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu')(x)\nx = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(x)\nx = Activation('relu')(x)\nx = MaxPooling2D(pool_size=2)(x)\n\nx = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(x)\nx = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(x)\nx = MaxPooling2D(pool_size=2)(x)\n\n# cifar10의 클래스가 10개 이므로 마지막 classification의 Dense layer units갯수는 10\nx = Flatten(name='flatten')(x)\nx = Dropout(rate=0.5)(x)\nx = Dense(300, activation='relu', name='fc1')(x)\nx = Dropout(rate=0.3)(x)\noutput = Dense(10, activation='softmax', name='output')(x)\n\nmodel = Model(inputs=input_tensor, outputs=output)\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimizer는 Adam으로 설정하고, label값이 원-핫 인코딩이 아니므로 loss는 sparse_categorical_crossentropy 임. \nmodel.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model 학습 수행 및 테스트 데이터로 평가 \n* Model의 fit() 메소드를 호출하여 학습\n* fit()은 학습 데이터가 Numpy array 자체로 들어올때, Generator 형태로 들어올때 약간의 수행로직 차이가 있음. \n* 인자로 x에는 학습 image data, y는 학습 label 데이터. \n* batch_size는 한번에 가져올 image/label array 갯수. 1개씩 가져오면 수행속도가 너무 느리고, 전체를 가져오면 GPU Memory 부족이 발생할 수 있으므로 적절한 batch_size 설정이 필요. 만약 학습 데이터가 generator일 경우, fit()에서 batch_size를 설정하지 않음. \n* epochs 는 전체 학습 데이터 학습을 반복 수행할 횟수\n* steps_per_epoch는 전체 학습 데이터를 몇번 배치 작업으로 수행하는가를 의미. 보통 입력데이터가 generator일 경우 설정. \n* validation_data는 검증용 데이터 세트\n* validation_steps는 검증용 데이터의 steps_per_epoch임. \n* validation_split는 validation_data로 별도의 검증용 데이터 세트를 설정하지 않고 자동으로 학습용 데이터에서 검증용 데이터 세트 분할. \n","metadata":{}},{"cell_type":"code","source":"history = model.fit(x=train_images, y=train_labels, batch_size=64, epochs=30, validation_split=0.15 )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\ndef show_history(history):\n    plt.figure(figsize=(6, 6))\n    plt.yticks(np.arange(0, 1, 0.05))\n    plt.plot(history.history['accuracy'], label='train')\n    plt.plot(history.history['val_accuracy'], label='valid')\n    plt.legend()\n    \nshow_history(history)\n\n# 테스트 데이터로 성능 평가\nmodel.evaluate(test_images, test_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### model.predict()를 통해 이미지 분류 예측\n* 4차원 이미지 배열을 입력해서 모델학습함. predict()시에도 4차원 이미지 배열을 입력해야함. \n* 학습 데이터의 원-핫 인코딩 적용 여부와 관계없이 softmax 적용 결과는 무조건 2차원 임에 유의  ","metadata":{}},{"cell_type":"code","source":"# 아래 코드는 오류 발생. Conv2D를 사용한 모델에 4차원 이미지 배열을 입력해서 모델을 학습했으므로 predict()시에도 테스트용 4차원 이미지 배열을 입력해야 함.  \npreds = model.predict(test_images[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 테스트용 4차원 이미지 배열을 입력해서 predict()수행. \n# predict()의 결과는 softmax 적용 결과임. 학습 데이터의 원-핫 인코딩 적용 여부와 관계없이 softmax 적용 결과는 무조건 2차원 임에 유의 \npreds = model.predict(np.expand_dims(test_images[0], axis=0))\nprint('예측 결과 shape:', preds.shape)\nprint('예측 결과:', preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = model.predict(test_images[:32], batch_size=32)\nprint('예측 결과 shape:', preds.shape)\nprint('예측 결과:', preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_class = np.argmax(preds, axis=1)\nprint('예측 클래스 값:', predicted_class)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_images(test_images[:8], predicted_class[:8], ncols=8)\nshow_images(test_images[:8], test_labels[:8], ncols=8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 평균이 1 이고 표준편차가 1인 표준 정규분포에서 난수 추출\n* 표준 편차가 클 수록 개별 값의 크기가 일반적으로 커짐.","metadata":{}},{"cell_type":"code","source":"numbers = np.random.normal(loc=0.0,scale=1,size=[100, 100])\nprint(numbers)\nprint(numbers.mean())\nprint(numbers.std())\nprint(numbers.sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Xavier initialization - 정규분포(glorot_normal), 균일분포(glorot_uniform) ","metadata":{}},{"cell_type":"code","source":"# glorot_normal\nfan_in = 20\nfan_out = 15\nscale_value = np.sqrt(2/(fan_in + fan_out))\nprint('scale:', scale_value)\nweights = np.random.normal(loc=0.0, scale=scale_value, size=(100, 100))\nprint(weights)\nprint('weights mean:',weights.mean(), 'std:', weights.std(), 'sum:', weights.sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# glorot_uniform\nfan_in = 10\nfan_out = 8\nlimit = np.sqrt(6/(fan_in + fan_out))\nprint('limit:', limit)\nweights = np.random.uniform(-1*limit, limit, size=(100, 100))\nprint(weights)\nprint('weights mean:',weights.mean(), 'std:', weights.std(), 'sum:', weights.sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### He initialization - 정규분포(he_normal), 균일분포(he_uniform) ","metadata":{}},{"cell_type":"code","source":"fan_in = 10\nfan_out = 8\nscale_value = np.sqrt(2/(fan_in))\nprint('scale:', scale_value)\nweights = np.random.normal(loc=0.0, scale=scale_value, size=(100, 100))\nprint(weights)\nprint('weights mean:',weights.mean(), 'std:', weights.std(), 'sum:', weights.sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fan_in = 10\nfan_out = 8\nlimit = np.sqrt(6/(fan_in))\nprint('limit:', limit)\nweights = np.random.uniform(-1*limit, limit, size=(100, 100))\nprint(weights)\nprint('weights mean:',weights.mean(), 'std:', weights.std(), 'sum:', weights.sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### weight 초기화를 He Normal로 변경 후 성능 검증\n* Keras Conv2D의 기본 weight 초기화는 glorot_uniform임. 이를 he_normal로 변경 후 동일 모델로 성능 테스트 \n* label은 원-핫 인코딩을 적용 ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.utils import to_categorical\n\n\ndef get_preprocessed_data(images, labels):\n    \n    # 학습과 테스트 이미지 array를 0~1 사이값으로 scale 및 float32 형 변형. \n    images = np.array(images/255.0, dtype=np.float32)\n    labels = np.array(labels, dtype=np.float32)\n    labels = labels.squeeze()\n    \n    return images, labels\n\n# 0 ~ 1사이값 float32로 변경하는 함수 호출 한 뒤 OHE 적용 \ndef get_preprocessed_ohe(images, labels):\n    images, labels = get_preprocessed_data(images, labels)\n    # OHE 적용 \n    oh_labels = to_categorical(labels)\n    return images, oh_labels\n\n(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n\ntrain_images, train_oh_labels = get_preprocessed_ohe(train_images, train_labels)\ntest_images, test_oh_labels = get_preprocessed_ohe(test_images, test_labels)\nprint(train_images.shape, train_oh_labels.shape, test_images.shape, test_oh_labels.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Dense , Conv2D , Dropout , Flatten , Activation, MaxPooling2D , GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam , RMSprop \nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\n\ninput_tensor = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n\n#x = Conv2D(filters=32, kernel_size=(5, 5), padding='valid', activation='relu')(input_tensor)\nx = Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu', kernel_initializer='he_normal')(input_tensor)\nx = Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu', kernel_initializer='he_normal')(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\nx = Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal')(x)\nx = Conv2D(filters=64, kernel_size=3, padding='same', kernel_initializer='he_normal')(x)\nx = Activation('relu')(x)\nx = MaxPooling2D(pool_size=2)(x)\n\nx = Conv2D(filters=128, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal')(x)\nx = Conv2D(filters=128, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal')(x)\nx = MaxPooling2D(pool_size=2)(x)\n\n# cifar10의 클래스가 10개 이므로 마지막 classification의 Dense layer units갯수는 10\nx = Flatten(name='flatten')(x)\nx = Dropout(rate=0.5)(x)\nx = Dense(300, activation='relu', name='fc1')(x)\nx = Dropout(rate=0.3)(x)\noutput = Dense(10, activation='softmax', name='output')(x)\n\nmodel = Model(inputs=input_tensor, outputs=output)\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimizer는 Adam으로 설정하고, label값이 원-핫 인코딩이므로 loss는 categorical_crossentropy 임. \nmodel.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(x=train_images, y=train_oh_labels, batch_size=64, epochs=30, validation_split=0.15 )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_history(history)\n\n# 테스트 데이터로 성능 평가\nmodel.evaluate(test_images, test_oh_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Batch Normalization을 모델에 적용 후 성능 검증","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport random as python_random\n\nnp.random.seed(2021)\npython_random.seed(2021)\ntf.random.set_seed(2021)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Dense , Conv2D , Dropout , Flatten , Activation, MaxPooling2D , GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam , RMSprop \nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\n\ninput_tensor = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n\n#x = Conv2D(filters=32, kernel_size=(5, 5), padding='valid', activation='relu')(input_tensor)\nx = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(input_tensor)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\nx = Conv2D(filters=64, kernel_size=3, padding='same')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = Conv2D(filters=64, kernel_size=3, padding='same')(x)\nx = Activation('relu')(x)\nx = Activation('relu')(x)\nx = MaxPooling2D(pool_size=2)(x)\n\nx = Conv2D(filters=128, kernel_size=3, padding='same')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = Conv2D(filters=128, kernel_size=3, padding='same')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = MaxPooling2D(pool_size=2)(x)\n\n# cifar10의 클래스가 10개 이므로 마지막 classification의 Dense layer units갯수는 10\nx = Flatten(name='flatten')(x)\nx = Dropout(rate=0.5)(x)\nx = Dense(300, activation='relu', name='fc1')(x)\nx = Dropout(rate=0.3)(x)\noutput = Dense(10, activation='softmax', name='output')(x)\n\nmodel = Model(inputs=input_tensor, outputs=output)\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# label값이 원-핫 인코딩이 아니므로 loss는 categorical_crossentropy 임. \nmodel.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(x=train_images, y=train_oh_labels, batch_size=64, epochs=30, validation_split=0.15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(test_images, test_oh_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### He Normal 적용 후 Batch Normalization","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Dense , Conv2D , Dropout , Flatten , Activation, MaxPooling2D , GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam , RMSprop \nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\n\ninput_tensor = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n\n#x = Conv2D(filters=32, kernel_size=(5, 5), padding='valid', activation='relu')(input_tensor)\nx = Conv2D(filters=32, kernel_size=(3, 3), padding='same', kernel_initializer='he_normal')(input_tensor)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = Conv2D(filters=32, kernel_size=(3, 3), padding='same', kernel_initializer='he_normal')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\nx = Conv2D(filters=64, kernel_size=3, padding='same', kernel_initializer='he_normal')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = Conv2D(filters=64, kernel_size=3, padding='same', kernel_initializer='he_normal')(x)\nx = Activation('relu')(x)\nx = Activation('relu')(x)\nx = MaxPooling2D(pool_size=2)(x)\n\nx = Conv2D(filters=128, kernel_size=3, padding='same', kernel_initializer='he_normal')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = Conv2D(filters=128, kernel_size=3, padding='same', kernel_initializer='he_normal')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = MaxPooling2D(pool_size=2)(x)\n\n# cifar10의 클래스가 10개 이므로 마지막 classification의 Dense layer units갯수는 10\nx = Flatten(name='flatten')(x)\nx = Dropout(rate=0.5)(x)\nx = Dense(300, activation='relu', name='fc1')(x)\nx = Dropout(rate=0.3)(x)\noutput = Dense(10, activation='softmax', name='output')(x)\n\nmodel = Model(inputs=input_tensor, outputs=output)\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(x=train_images, y=train_oh_labels, batch_size=64, epochs=30, validation_split=0.15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(test_images, test_oh_labels, batch_size=64)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = model.predict(np.expand_dims(test_images[0], axis=0))\npredicted_class = np.argmax(preds, axis=1)\nprint('예측 클래스 값:', predicted_class)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 학습 시 데이터를 섞는 shuffle 적용 유무에 따른  성능 테스트","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport random as python_random\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\n# seed 를 설정해서 학습시마다 동일한 결과 유도. 불행히도 의도한 대로 동작하지 않음. \ndef set_random_seed(seed_value):\n    np.random.seed(seed_value)\n    python_random.seed(seed_value)\n    tf.random.set_seed(seed_value)\n\n# 0 ~ 1사이값의 float32로 변경하는 함수\ndef get_preprocessed_data(images, labels):\n    \n    # 학습과 테스트 이미지 array를 0~1 사이값으로 scale 및 float32 형 변형. \n    images = np.array(images/255.0, dtype=np.float32)\n    labels = np.array(labels, dtype=np.float32)\n    \n    return images, labels\n\n# 0 ~ 1사이값 float32로 변경하는 함수 호출 한 뒤 OHE 적용 \ndef get_preprocessed_ohe(images, labels):\n    images, labels = get_preprocessed_data(images, labels)\n    # OHE 적용 \n    oh_labels = to_categorical(labels)\n    return images, oh_labels\n\n# 학습/검증/테스트 데이터 세트에 전처리 및 OHE 적용한 뒤 반환 \ndef get_train_valid_test_set(train_images, train_labels, test_images, test_labels, valid_size=0.15, random_state=2021):\n    # 학습 및 테스트 데이터 세트를  0 ~ 1사이값 float32로 변경 및 OHE 적용. \n    train_images, train_oh_labels = get_preprocessed_ohe(train_images, train_labels)\n    test_images, test_oh_labels = get_preprocessed_ohe(test_images, test_labels)\n    \n    # 학습 데이터를 검증 데이터 세트로 다시 분리\n    tr_images, val_images, tr_oh_labels, val_oh_labels = train_test_split(train_images, train_oh_labels, test_size=valid_size, random_state=random_state)\n    \n    return (tr_images, tr_oh_labels), (val_images, val_oh_labels), (test_images, test_oh_labels ) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.datasets import cifar10\n\n# random seed는 2021로 고정.\nset_random_seed(2021)\n# CIFAR10 데이터 재 로딩 및 Scaling/OHE 전처리 적용하여 학습/검증/데이터 세트 생성. \n(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\nprint(train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)\n(tr_images, tr_oh_labels), (val_images, val_oh_labels), (test_images, test_oh_labels) = \\\n    get_train_valid_test_set(train_images, train_labels, test_images, test_labels, valid_size=0.15, random_state=2021)\n\nprint(tr_images.shape, tr_oh_labels.shape, val_images.shape, val_oh_labels.shape, test_images.shape, test_oh_labels.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### model 생성을 위한 별도 함수 생성","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Dense , Conv2D , Dropout , Flatten , Activation, MaxPooling2D , GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam , RMSprop \nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\n\ndef create_model():\n    input_tensor = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n\n    #x = Conv2D(filters=32, kernel_size=(5, 5), padding='valid', activation='relu')(input_tensor)\n    x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(input_tensor)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = Conv2D(filters=64, kernel_size=3, padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(filters=64, kernel_size=3, padding='same')(x)\n    x = Activation('relu')(x)\n    x = Activation('relu')(x)\n    x = MaxPooling2D(pool_size=2)(x)\n\n    x = Conv2D(filters=128, kernel_size=3, padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(filters=128, kernel_size=3, padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = MaxPooling2D(pool_size=2)(x)\n\n    # cifar10의 클래스가 10개 이므로 마지막 classification의 Dense layer units갯수는 10\n    x = Flatten(name='flatten')(x)\n    x = Dropout(rate=0.5)(x)\n    x = Dense(300, activation='relu', name='fc1')(x)\n    x = Dropout(rate=0.3)(x)\n    output = Dense(10, activation='softmax', name='output')(x)\n\n    model = Model(inputs=input_tensor, outputs=output)\n    #model.summary()\n    \n    return model\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### shuffle을 False/True 변경하면서 테스트 ","metadata":{}},{"cell_type":"code","source":"model = create_model()\nmodel.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n# 먼저 shuffle을 false로 테스트 \nnoshuffle_history = model.fit(x=tr_images, y=tr_oh_labels, batch_size=64, epochs=30, shuffle=False, \n                    validation_data=(val_images, val_oh_labels))\nevaluation_result = model.evaluate(test_images, test_oh_labels, batch_size=64)\nprint('#### 테스트 세트로 evaluation 결과 :', evaluation_result)\n\n# model이 반복적으로 메모리 차지하는것을 없애기 위해서 수행. \ntf.keras.backend.clear_session()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### shuffle을 True로 변경하고 학습 및 테스트","metadata":{}},{"cell_type":"code","source":"model = create_model()\nmodel.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n# shuffle을 True로 변경하여 학습 및 테스트\nshuffle_history = model.fit(x=tr_images, y=tr_oh_labels, batch_size=64, epochs=30, shuffle=True, \n                    validation_data=(val_images, val_oh_labels))\nevaluation_result = model.evaluate(test_images, test_oh_labels, batch_size=64)\nprint('#### 테스트 세트로 evaluation 결과 :', evaluation_result)\n\ntf.keras.backend.clear_session()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 위에서 수행한 Shuffle테스트 시 validation 데이터 기반 성능 검증 시각화 ","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\ndef show_history_shuffle(noshuffle_history, shuffle_history):\n    figure, axs = plt.subplots(nrows=1, ncols=2, figsize=(16, 4))\n    # shuffle과 no shuffle의 validation accuracy 비교 \n    axs[0].plot(noshuffle_history.history['val_accuracy'], label='no shuffle acc')\n    axs[0].plot(shuffle_history.history['val_accuracy'], label='shuffle acc')\n    # shuffle과 no shuffle의 validation loss 비교 \n    axs[1].plot(noshuffle_history.history['val_loss'], label='no shuffle loss')\n    axs[1].plot(shuffle_history.history['val_loss'], label='shuffle loss')\n    axs[0].legend()\n    axs[1].legend()\n\nshow_history_shuffle(noshuffle_history, shuffle_history)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### batch 크기를 32, 64, 256, 512로 변경하면서 테스트","metadata":{}},{"cell_type":"code","source":"b_sizes = [32, 64, 256, 512]\nhistories = []\nevaluations = []\nfor b_size in b_sizes:\n    model = create_model()\n    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n    # batch_size를 순차적으로 32, 64, 256, 512로 변경하여 학습 및 evaluation 수행. \n    print('##### batch size :', b_size, '학습 #####')\n    history = model.fit(x=tr_images, y=tr_oh_labels, batch_size=b_size, epochs=30, \n                        shuffle=True, validation_data=(val_images, val_oh_labels))\n    # batch size별 학습 history 결과 저장. \n    histories.append(history)\n    # 테스트 세트로 evaluation 수행하고 batch size별 결과 저장. \n    evaluation_result = model.evaluate(test_images, test_oh_labels, batch_size=b_size)\n    print('#### 테스트 세트로 evaluation 결과 :', evaluation_result)\n    evaluations.append(evaluation_result)\n    \n    tf.keras.backend.clear_session()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\ndef show_history_batch(histories):\n    figure, axs = plt.subplots(nrows=1, ncols=2, figsize=(16, 4))  \n    # batch 크기별 validation accuracy 비교 \n    axs[0].plot(histories[0].history['val_accuracy'], label='batch 32 acc')\n    axs[0].plot(histories[1].history['val_accuracy'], label='batch 64 acc')\n    axs[0].plot(histories[2].history['val_accuracy'], label='batch 256 acc')\n    axs[0].plot(histories[3].history['val_accuracy'], label='batch 512 acc')\n    \n    # batch 크기별 validation loss 비교\n    axs[1].plot(histories[0].history['val_loss'], label='batch 32 loss')\n    axs[1].plot(histories[1].history['val_loss'], label='batch 64 loss')\n    axs[1].plot(histories[2].history['val_loss'], label='batch 256 loss')\n    axs[1].plot(histories[3].history['val_loss'], label='batch 512 loss')\n    \n    axs[0].legend()\n    axs[1].legend()\n\nshow_history_batch(histories)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}